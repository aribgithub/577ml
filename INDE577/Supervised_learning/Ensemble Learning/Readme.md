Ensemble machine learning

In the world of machine learning, it is full of different levels of algorithms. However, when applying these algorithms, something is missing with data due to it being bias or variance or noise in data. We are able to find the variance or loss in the result, but unable to find a good accuracy. We can use ensemble machine learning to deal with these problems.

Ensemble learning combines several tree base algorithms which construct a better predictive performance than a single tree base algorithm. The main principle of ensemble model is that several weak learners combined together to form a strong learner resulting in an increasing accuracy model.
![68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f313339362f312a75336163546363665a32485957696c67674f6d4643412e6a706567](https://user-images.githubusercontent.com/119718873/205553905-b920e186-ae88-4a91-9bc7-1d166632375e.jpeg)

Bagging

Another esemble learning method is bagging. Bagging is based on a bootstrapping sampling technique. Bootstrapping creates multiple sets of the original training data with replacement. Replacement enables the duplication of sample instances in a set. Each subset has the same equal size and can be used to train models in parallel.

Random Forest

Random Forest is an ensemble of Decision Trees, generally trained via the bagging method. The main difference between the decision tree algorithm and the random forest algorithm is that establishing root nodes and segregating nodes is done randomly in the latter. The random forest employs the bagging method to generate the required prediction.

